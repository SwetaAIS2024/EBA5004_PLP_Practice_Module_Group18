{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJbY49MgBetj",
        "outputId": "1a71ecd7-8fbb-4a49-cb9e-efaacfcb9d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.8)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.11/dist-packages (6.8.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.56)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.39)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.9)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (0.14.3)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.20.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (1.17.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (2.22)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain-community sentence-transformers gradio python-dotenv google-generativeai langchain-google-genai chromadb firebase-admin langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HhyqTZwcBTnR",
        "outputId": "3dc2806a-db53-4397-9308-631b5c052ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] --- Configuration ---\n",
            "[INFO] Loading environment variables...\n",
            "[INFO] Environment variables loaded.\n",
            "[INFO] Configured Chroma DB persistence path: /content/drive/MyDrive/chroma_dbs/alumni_chatbot_gemini_firestore\n",
            "[INFO] Configured Firestore key path (needed for rebuild): /content/firestore_key.json\n",
            "[INFO] Configured Firestore collection name (needed for rebuild): alumni_profiles\n",
            "[INFO] Configured Embedding Model: models/embedding-001\n",
            "[INFO] Configured LLM Model: gemini-1.5-flash\n",
            "[INFO] --- End Configuration ---\n",
            "\n",
            "--- Starting Enhanced Alumni Chatbot ---\n",
            "[INFO] Checking environment...\n",
            "[INFO] Running in Google Colab environment.\n",
            "✅ Google Drive already mounted.\n",
            "✅ Ensured Chroma DB path exists: /content/drive/MyDrive/chroma_dbs/alumni_chatbot_gemini_firestore\n",
            "\n",
            "[INFO] --- Initializing Components ---\n",
            "[INFO] Attempting to load Google API Key...\n",
            "✅ Google API Key loaded from Colab Secrets.\n",
            "[INFO] Attempting to initialize Firebase Admin and connect to Firestore...\n",
            "✅ Firebase app already initialized.\n",
            "✅ Firestore client obtained.\n",
            "[INFO] Attempting to initialize embeddings model: models/embedding-001\n",
            "✅ Embeddings model (models/embedding-001) initialized successfully.\n",
            "[INFO] Setting up vector store...\n",
            "[INFO] Ensuring persistence directory exists: /content/drive/MyDrive/chroma_dbs/alumni_chatbot_gemini_firestore\n",
            "[INFO] Attempting to load existing vector store from /content/drive/MyDrive/chroma_dbs/alumni_chatbot_gemini_firestore...\n",
            "✅ Vector store loaded successfully with 12526 documents.\n",
            "[INFO] Attempting to initialize LLM: gemini-1.5-flash\n",
            "[INFO] Testing LLM connection with a simple invoke...\n",
            "✅ LLM test invoke successful.\n",
            "✅ LLM (gemini-1.5-flash) initialized successfully.\n",
            "[INFO] Setting up RAG chain...\n",
            "✅ Custom filtered retriever created.\n",
            "✅ Conversation buffer memory initialized.\n",
            "[INFO] Using current date for prompt: 2025-05-09\n",
            "✅ Prompt template created.\n",
            "✅ Enhanced RAG chain setup complete.\n",
            "[INFO] --- Initialization Complete ---\n",
            "\n",
            "[INFO] Launching Gradio interface...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-a049887851e7>:951: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a0d62bf80bc18c3722.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a0d62bf80bc18c3722.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Received user query: 'can you give information about John carter ? '\n",
            "[INFO] Attempting to extract filters from query: 'can you give information about John carter ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'can you give information about John carter ?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'can you give information about John carter ?'\n",
            "[INFO] Searching vector store with query: 'can you give information about John carter ?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for can you give information about John carter ?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for can you give information about John carter ?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 5 unique documents from 3 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 5 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'John Carter's contact information is:\n",
            "\n",
            "* Email: jeffery26@example.org\n",
            "* Phone: (864)217-2320\n",
            "* Locat'\n",
            "[INFO] Retrieved 5 source documents.\n",
            "\n",
            "[INFO] Received user query: 'can you give information about John carter ? '\n",
            "[INFO] Attempting to extract filters from query: 'can you give information about John carter ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'What information is available about John Carter?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'What information is available about John Carter?'\n",
            "[INFO] Searching vector store with query: 'What information is available about John Carter?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for John Carter' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about John Carter's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'John Carter's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for What information is available about John Carter?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for What information is available about John Carter?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 16 unique documents from 6 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'John Carter's contact information is:\n",
            "\n",
            "* Email: jeffery26@example.org\n",
            "* Phone: (864)217-2320\n",
            "* Locat'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Find alumni proficient in Data analysis ?'\n",
            "[INFO] Attempting to extract filters from query: 'Find alumni proficient in Data analysis ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni are proficient in data analysis?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni are proficient in data analysis?'\n",
            "[INFO] Searching vector store with query: 'Which alumni are proficient in data analysis?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni are proficient in data analysis?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni are proficient in data analysis?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 7 unique documents from 3 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 7 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'Based on the provided text, the following alumni are proficient in data analysis:  All alumni mentio'\n",
            "[INFO] Retrieved 7 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Who has experience with cloud computing?'\n",
            "[INFO] Attempting to extract filters from query: 'Who has experience with cloud computing?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni have experience with cloud computing?'\n",
            "[INFO] Searching vector store with query: 'Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about experience skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about cloud computing skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 36 unique documents from 9 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'Based on the provided information, the following alumni have experience with cloud computing:\n",
            "\n",
            "* Alu'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Who has experience with cloud computing? share their name and contact information ? '\n",
            "[INFO] Attempting to extract filters from query: 'Who has experience with cloud computing? share their name and contact information ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni have experience with cloud computing, and what are their names and contact details?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni have experience with cloud computing, and what are their names and contact details?'\n",
            "[INFO] Searching vector store with query: 'Which alumni have experience with cloud computing, and what are their names and contact details?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in contact' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning contact' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about contact skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about experience skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about cloud computing skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni have experience with cloud computing, and what are their names and contact details?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni have experience with cloud computing, and what are their names and contact details?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 48 unique documents from 12 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'Based on the provided information, several alumni have experience with cloud computing, but their na'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Who has experience with cloud computing? share their contact information ?'\n",
            "[INFO] Attempting to extract filters from query: 'Who has experience with cloud computing? share their contact information ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni have experience with cloud computing, and what is their contact information?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni have experience with cloud computing, and what is their contact information?'\n",
            "[INFO] Searching vector store with query: 'Which alumni have experience with cloud computing, and what is their contact information?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in contact' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning contact' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about contact skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about experience skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about cloud computing skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni have experience with cloud computing, and what is their contact information?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni have experience with cloud computing, and what is their contact information?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 53 unique documents from 12 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'Based on the provided information, several alumni have experience with cloud computing, but their na'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Who has experience with cloud computing?'\n",
            "[INFO] Attempting to extract filters from query: 'Who has experience with cloud computing?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni have experience with cloud computing?'\n",
            "[INFO] Searching vector store with query: 'Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about experience skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning cloud computing' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about cloud computing skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni have experience with cloud computing?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 36 unique documents from 9 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'Based on the provided information, several alumni have experience with cloud computing, but their na'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Tell me about Paul Walker's work experience.'\n",
            "[INFO] Attempting to extract filters from query: 'Tell me about Paul Walker's work experience.'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'What is Paul Walker's work experience?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'What is Paul Walker's work experience?'\n",
            "[INFO] Searching vector store with query: 'What is Paul Walker's work experience?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in work' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning work' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about work skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about experience skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Paul Walker' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Paul Walker's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Paul Walker's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for What is Paul Walker's work experience?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for What is Paul Walker's work experience?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 41 unique documents from 12 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'I cannot find information about Paul Walker's work experience in the provided text.'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Find alumni with Python programming skills.'\n",
            "[INFO] Attempting to extract filters from query: 'Find alumni with Python programming skills.'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni have Python programming skills?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni have Python programming skills?'\n",
            "[INFO] Searching vector store with query: 'Which alumni have Python programming skills?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in skills' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning skills' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about skills skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in Python' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning Python' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Python skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni have Python programming skills?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni have Python programming skills?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 32 unique documents from 9 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'Based on the provided information, the following alumni have Python programming skills:\n",
            "\n",
            "* One alumn'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Find alumni with computer engineering degree  skills who graduated after 2015.'\n",
            "[INFO] Attempting to extract filters from query: 'Find alumni with computer engineering degree  skills who graduated after 2015.'\n",
            "[INFO] Cleaned question for RAG: 'Find alumni with computer engineering degree skills who graduated after 2015.'\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'Which alumni have computer engineering degrees and graduated after 2015?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'Which alumni have computer engineering degrees and graduated after 2015?'\n",
            "[INFO] Searching vector store with query: 'Which alumni have computer engineering degrees and graduated after 2015?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for Which alumni have computer engineering degrees and graduated after 2015?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for Which alumni have computer engineering degrees and graduated after 2015?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 8 unique documents from 3 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 8 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'All the alumni listed have computer science degrees, not computer engineering degrees.  The followin'\n",
            "[INFO] Retrieved 8 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Share their company information and contact information ? '\n",
            "[INFO] Attempting to extract filters from query: 'Share their company information and contact information ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'What are the company names and contact details for Mathew Campbell, Paul Ortega, Kevin Rodriguez, Jennifer Sullivan, Angela Robinson, Eric Rodriguez, Mrs. Erica Harris, and Jennifer Young?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'What are the company names and contact details for Mathew Campbell, Paul Ortega, Kevin Rodriguez, Jennifer Sullivan, Angela Robinson, Eric Rodriguez, Mrs. Erica Harris, and Jennifer Young?'\n",
            "[INFO] Searching vector store with query: 'What are the company names and contact details for Mathew Campbell, Paul Ortega, Kevin Rodriguez, Jennifer Sullivan, Angela Robinson, Eric Rodriguez, Mrs. Erica Harris, and Jennifer Young?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Alumni with expertise in contact' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profiles mentioning contact' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about contact skills/experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Kevin Rodriguez' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Kevin Rodriguez's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Kevin Rodriguez's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Eric Rodriguez' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Eric Rodriguez's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Eric Rodriguez's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Jennifer Sullivan' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Jennifer Sullivan's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Jennifer Sullivan's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Paul Ortega' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Paul Ortega's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Paul Ortega's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Erica Harris' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Erica Harris's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Erica Harris's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Jennifer Young' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Jennifer Young's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Jennifer Young's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Angela Robinson' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Angela Robinson's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Angela Robinson's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Profile details for Mathew Campbell' and filter: None\n",
            "[INFO] Searching vector store with query: 'Information about Mathew Campbell's background' and filter: None\n",
            "[INFO] Searching vector store with query: 'Mathew Campbell's skills and experience' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for What are the company names and contact details for Mathew Campbell, Paul Ortega, Kevin Rodriguez, Jennifer Sullivan, Angela Robinson, Eric Rodriguez, Mrs. Erica Harris, and Jennifer Young?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for What are the company names and contact details for Mathew Campbell, Paul Ortega, Kevin Rodriguez, Jennifer Sullivan, Angela Robinson, Eric Rodriguez, Mrs. Erica Harris, and Jennifer Young?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 106 unique documents from 30 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 10 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'I am sorry, but the provided text does not contain contact details for Mathew Campbell, Paul Ortega,'\n",
            "[INFO] Retrieved 10 source documents.\n",
            "\n",
            "[INFO] Received user query: 'Find alumni with computer engineering degree skills who graduated after 2015. ? Share their company information ?'\n",
            "[INFO] Attempting to extract filters from query: 'Find alumni with computer engineering degree skills who graduated after 2015. ? Share their company information ?'\n",
            "[INFO] No filters were successfully extracted or applied.\n",
            "[INFO] No metadata filters were applied.\n",
            "[INFO] CustomFilteredRetriever called with query: 'What company information is available for alumni with computer engineering degrees who graduated after 2015?' and filter: None\n",
            "[INFO] Expanding query for RAG retrieval: 'What company information is available for alumni with computer engineering degrees who graduated after 2015?'\n",
            "[INFO] Searching vector store with query: 'What company information is available for alumni with computer engineering degrees who graduated after 2015?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Search results for What company information is available for alumni with computer engineering degrees who graduated after 2015?' and filter: None\n",
            "[INFO] Searching vector store with query: 'Relevant information for What company information is available for alumni with computer engineering degrees who graduated after 2015?' and filter: None\n",
            "[INFO] CustomFilteredRetriever collected 8 unique documents from 3 expanded queries.\n",
            "✅ CustomFilteredRetriever returning top 8 documents after combining and re-ranking.\n",
            "✅ RAG chain invoke successful.\n",
            "[INFO] Received answer (first 100 chars): 'The provided text does not contain information about alumni with computer engineering degrees.  All '\n",
            "[INFO] Retrieved 8 source documents.\n"
          ]
        }
      ],
      "source": [
        "# Import statements\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import gradio as gr\n",
        "import shutil\n",
        "import traceback\n",
        "import datetime\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# --- Configuration ---\n",
        "print(\"[INFO] --- Configuration ---\")\n",
        "print(\"[INFO] Loading environment variables...\")\n",
        "try:\n",
        "    load_dotenv()\n",
        "    print(\"[INFO] Environment variables loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Failed to load environment variables: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Google Drive Configuration for Persistence ---\n",
        "DRIVE_CHROMA_DIR = \"/content/drive/MyDrive/chroma_dbs/alumni_chatbot_gemini_firestore\"\n",
        "VECTOR_DB_PATH = DRIVE_CHROMA_DIR\n",
        "print(f\"[INFO] Configured Chroma DB persistence path: {VECTOR_DB_PATH}\")\n",
        "\n",
        "# --- Cloud Database Configuration ---\n",
        "FIRESTORE_KEY_PATH = '/content/firestore_key.json' # Ensure this path is correct in Colab or local setup\n",
        "print(f\"[INFO] Configured Firestore key path (needed for rebuild): {FIRESTORE_KEY_PATH}\")\n",
        "\n",
        "FIRESTORE_COLLECTION_NAME = os.getenv(\"FIRESTORE_COLLECTION_NAME\", \"alumni_profiles\")\n",
        "print(f\"[INFO] Configured Firestore collection name (needed for rebuild): {FIRESTORE_COLLECTION_NAME}\")\n",
        "\n",
        "# --- Model Configuration ---\n",
        "EMBEDDING_MODEL = \"models/embedding-001\"\n",
        "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gemini-1.5-flash\") # Allow overriding LLM model via env var\n",
        "print(f\"[INFO] Configured Embedding Model: {EMBEDDING_MODEL}\")\n",
        "print(f\"[INFO] Configured LLM Model: {LLM_MODEL}\")\n",
        "print(\"[INFO] --- End Configuration ---\")\n",
        "\n",
        "# --- Global Variables ---\n",
        "vector_store = None\n",
        "rag_chain = None\n",
        "embeddings = None\n",
        "llm = None\n",
        "firestore_db = None\n",
        "google_api_key = None\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_google_api_key():\n",
        "    \"\"\"Loads the Google API Key securely from Colab Secrets or environment.\"\"\"\n",
        "    global google_api_key\n",
        "    print(\"[INFO] Attempting to load Google API Key...\")\n",
        "    try:\n",
        "        # Try Colab Secrets first\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            api_key = userdata.get('GOOGLE_API_KEY')\n",
        "            if api_key:\n",
        "                print(\"✅ Google API Key loaded from Colab Secrets.\")\n",
        "                google_api_key = api_key\n",
        "                return api_key\n",
        "            else:\n",
        "                 print(\"⚠️ Colab Secrets available, but 'GOOGLE_API_KEY' not found.\")\n",
        "        except ImportError:\n",
        "            print(\"⚠️ Not running in Google Colab or `userdata` not available.\")\n",
        "\n",
        "        # Fallback to environment variable\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "        if api_key:\n",
        "            print(\"✅ Google API Key loaded from environment variable.\")\n",
        "            google_api_key = api_key\n",
        "            return api_key\n",
        "\n",
        "        print(\"❌ Google API Key not found in environment variables or Colab Secrets. Please set GOOGLE_API_KEY.\")\n",
        "        google_api_key = None\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading API key: {e}\")\n",
        "        traceback.print_exc()\n",
        "        google_api_key = None\n",
        "        return None\n",
        "\n",
        "def initialize_database():\n",
        "    \"\"\"Initializes the connection to Firestore.\"\"\"\n",
        "    global firestore_db\n",
        "    print(\"[INFO] Attempting to initialize Firebase Admin and connect to Firestore...\")\n",
        "    try:\n",
        "        if not os.path.exists(FIRESTORE_KEY_PATH):\n",
        "            print(f\"❌ Firestore key file not found at {FIRESTORE_KEY_PATH}. Cannot initialize database.\")\n",
        "            firestore_db = None\n",
        "            return\n",
        "\n",
        "        if not firebase_admin._apps:\n",
        "            print(\"[INFO] Firebase app not initialized. Proceeding with initialization.\")\n",
        "            try:\n",
        "                cred = credentials.Certificate(FIRESTORE_KEY_PATH)\n",
        "                firebase_admin.initialize_app(cred)\n",
        "                print(\"✅ Firebase Admin initialized.\")\n",
        "            except Exception as init_e:\n",
        "                print(f\"❌ Failed to initialize Firebase Admin app: {init_e}\")\n",
        "                traceback.print_exc()\n",
        "                firestore_db = None\n",
        "                return\n",
        "        else:\n",
        "            print(\"✅ Firebase app already initialized.\")\n",
        "\n",
        "        firestore_db = firestore.client()\n",
        "        print(\"✅ Firestore client obtained.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to initialize Firebase Admin or Firestore client: {e}\")\n",
        "        traceback.print_exc()\n",
        "        firestore_db = None\n",
        "\n",
        "def initialize_embeddings():\n",
        "    \"\"\"Initializes the embedding model.\"\"\"\n",
        "    global embeddings, google_api_key\n",
        "    print(f\"[INFO] Attempting to initialize embeddings model: {EMBEDDING_MODEL}\")\n",
        "    try:\n",
        "        if google_api_key is None:\n",
        "            print(\"[INFO] API key not loaded yet, attempting to load...\")\n",
        "            google_api_key = load_google_api_key()\n",
        "            if google_api_key is None:\n",
        "                print(\"❌ Cannot initialize embeddings: Google API Key not available.\")\n",
        "                embeddings = None\n",
        "                return\n",
        "\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "        print(f\"✅ Embeddings model ({EMBEDDING_MODEL}) initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to initialize embeddings model: {e}\")\n",
        "        traceback.print_exc()\n",
        "        embeddings = None\n",
        "\n",
        "def initialize_llm():\n",
        "    \"\"\"Initializes the LLM.\"\"\"\n",
        "    global llm, google_api_key\n",
        "    print(f\"[INFO] Attempting to initialize LLM: {LLM_MODEL}\")\n",
        "    try:\n",
        "        if google_api_key is None:\n",
        "             print(\"[INFO] API key not loaded yet, attempting to load...\")\n",
        "             google_api_key = load_google_api_key()\n",
        "             if google_api_key is None:\n",
        "                print(\"❌ Cannot initialize LLM: Google API Key not available.\")\n",
        "                llm = None\n",
        "                return\n",
        "\n",
        "        # Using convert_system_message_to_human=True will raise a deprecation warning.\n",
        "        # Consider removing this parameter if not strictly needed, or be aware it will\n",
        "        # need removal/code adjustment in a future library version.\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=LLM_MODEL,\n",
        "            google_api_key=google_api_key,\n",
        "            temperature=0.2,\n",
        "            # convert_system_message_to_human=True # Keep for now to match original, but note the warning\n",
        "        )\n",
        "\n",
        "        # Test connection\n",
        "        try:\n",
        "            print(f\"[INFO] Testing LLM connection with a simple invoke...\")\n",
        "            llm.invoke(\"Hello\")\n",
        "            print(\"✅ LLM test invoke successful.\")\n",
        "            print(f\"✅ LLM ({LLM_MODEL}) initialized successfully.\")\n",
        "        except Exception as api_check_e:\n",
        "            print(f\"❌ Failed LLM test invoke. This might indicate an API key issue, billing issue, or quota limit: {api_check_e}\")\n",
        "            traceback.print_exc()\n",
        "            llm = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during LLM initialization: {e}\")\n",
        "        traceback.print_exc()\n",
        "        llm = None\n",
        "\n",
        "# --- Enhanced Text Processing ---\n",
        "def chunk_text(text: str) -> List[Document]:\n",
        "    \"\"\"Improved text chunking with better context preservation.\"\"\"\n",
        "    print(f\"[INFO] Chunking text (length: {len(text)})...\")\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        print(\"[WARN] Cannot chunk empty or non-string text.\")\n",
        "        return []\n",
        "    try:\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=800,  # Smaller chunks for better precision\n",
        "            chunk_overlap=200,  # More overlap for context\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Better splitting\n",
        "        )\n",
        "        # Wrap text in a Document object for the splitter\n",
        "        chunks = splitter.split_documents([Document(page_content=text)])\n",
        "        print(f\"✅ Created {len(chunks)} chunks.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during text chunking: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "def extract_education_info(education_text: str) -> Tuple[Optional[int], List[str]]:\n",
        "    \"\"\"Enhanced education info extraction with degree parsing.\"\"\"\n",
        "    # print(f\"[INFO] Extracting education info from: {education_text[:100]}...\") # Log partial text\n",
        "    graduation_year = None\n",
        "    degrees = []\n",
        "\n",
        "    if not isinstance(education_text, str):\n",
        "        print(\"[WARN] Education text is not a string.\")\n",
        "        return None, []\n",
        "\n",
        "    # Extract degree information and years using a more flexible pattern\n",
        "    # Pattern looks for (TEXT) followed by (YYYY-MM-DD - YYYY-MM-DD) or similar date formats\n",
        "    # Adjusted pattern to be more robust to variations, capturing text before dated parenthesis\n",
        "    degree_pattern = r\"([^()]+)\\s*\\((?:[^()]*?)?\\b(\\d{4})-\\d{2}-\\d{2}\\b\\s*-\\s*\\b(\\d{4})-\\d{2}-\\d{2}\\b(?:[^()]*?)?\\)\"\n",
        "    matches = re.findall(degree_pattern, education_text)\n",
        "\n",
        "    years = []\n",
        "    for match in matches:\n",
        "        degree_part = match[0].strip()\n",
        "        # Assume degree name is the captured text before the dated parenthesis\n",
        "        degree_name = degree_part\n",
        "\n",
        "        # Use the second captured year (end year)\n",
        "        end_year_str = match[2].strip()\n",
        "        try:\n",
        "            end_year = int(end_year_str)\n",
        "            years.append(end_year)\n",
        "            degrees.append(f\"{degree_name} (completed {end_year})\")\n",
        "        except ValueError:\n",
        "            print(f\"[WARN] Could not parse integer year from date string: {end_year_str}\")\n",
        "            degrees.append(f\"{degree_name} (date unknown)\") # Append degree even if year parsing fails\n",
        "\n",
        "    # If the first pattern didn't find matches, try a simpler one just looking for 4-digit years\n",
        "    if not years:\n",
        "         simple_year_pattern = r\"\\b(\\d{4})\\b\"\n",
        "         year_matches = re.findall(simple_year_pattern, education_text)\n",
        "         try:\n",
        "              if year_matches:\n",
        "                   # Assume the latest year found might be graduation year if no dated degrees\n",
        "                   graduation_year = max(int(y) for y in year_matches)\n",
        "                   print(f\"[INFO] Found years via simple pattern, assuming latest ({graduation_year}) is grad year.\")\n",
        "         except ValueError:\n",
        "              print(\"[WARN] Could not parse year from simple year pattern.\")\n",
        "\n",
        "    if years:\n",
        "        graduation_year = max(years)  # Use most recent graduation year from dated entries\n",
        "        # print(f\"✅ Extracted graduation year: {graduation_year}, Degrees: {degrees}\")\n",
        "    # else:\n",
        "        # print(\"INFO] No dated education entries found.\")\n",
        "        # If no dated entries, graduation_year remains None unless simple pattern found one.\n",
        "\n",
        "    return graduation_year, degrees\n",
        "\n",
        "# --- Data Loading with Enhanced Processing ---\n",
        "def load_data_from_firestore() -> List[Document]:\n",
        "    \"\"\"Enhanced data loading with better metadata extraction.\"\"\"\n",
        "    print(f\"[INFO] Loading data from Firestore collection: {FIRESTORE_COLLECTION_NAME}\")\n",
        "    if firestore_db is None:\n",
        "        print(\"❌ Cannot load data from Firestore: Database connection not initialized.\")\n",
        "        return []\n",
        "\n",
        "    documents = []\n",
        "    try:\n",
        "        collection_ref = firestore_db.collection(FIRESTORE_COLLECTION_NAME)\n",
        "        docs = collection_ref.stream()\n",
        "        doc_count = 0\n",
        "        processed_count = 0\n",
        "\n",
        "        print(\"[INFO] Streaming documents from Firestore...\")\n",
        "        for doc in docs:\n",
        "            doc_data = doc.to_dict()\n",
        "            alumni_id = doc.id\n",
        "            doc_count += 1\n",
        "            # print(f\"[INFO] Processing document ID: {alumni_id}\")\n",
        "\n",
        "            # Enhanced field processing\n",
        "            profile_text_parts = []\n",
        "            metadata = {\n",
        "                'alumni_id': alumni_id,\n",
        "                'name': doc_data.get('Name', 'Unknown Alumni'),\n",
        "                'Location': doc_data.get('Location', ''),\n",
        "                'Email': doc_data.get('Email', ''),\n",
        "                'Phone': doc_data.get('Phone', ''),\n",
        "                'major': doc_data.get('major', ''), # Assuming 'major' is a field\n",
        "                'Skills': doc_data.get('Skills', []), # Assuming 'Skills' is a list or comma-separated string\n",
        "                'WorkExperience': doc_data.get('WorkExperience', ''), # Assuming 'WorkExperience' is text\n",
        "                'Education': doc_data.get('Education', '') # Keep original education text in metadata\n",
        "            }\n",
        "\n",
        "            # Process education with enhanced extraction\n",
        "            education_text = doc_data.get('Education', '')\n",
        "            grad_year, degrees = extract_education_info(education_text)\n",
        "            if grad_year:\n",
        "                metadata['graduation_year_int'] = grad_year # Store as int for range filtering\n",
        "            if degrees:\n",
        "                metadata['degrees'] = degrees # Store parsed degrees\n",
        "                profile_text_parts.append(\"Education: \" + \"; \".join(degrees))\n",
        "            elif education_text: # If extraction didn't find dated degrees, but education field exists\n",
        "                 profile_text_parts.append(\"Education: \" + education_text) # Add raw text to content\n",
        "\n",
        "            # Process Skills field\n",
        "            skills_value = doc_data.get('Skills', '')\n",
        "            if isinstance(skills_value, list):\n",
        "                 # If it's already a list, use it directly for metadata and content\n",
        "                 metadata['Skills'] = skills_value\n",
        "                 if skills_value:\n",
        "                      profile_text_parts.append(f\"Skills: {', '.join(map(str, skills_value))}\")\n",
        "            elif isinstance(skills_value, str) and skills_value.strip():\n",
        "                 # If it's a string, add to content. Attempt parsing into list for metadata.\n",
        "                 metadata['Skills'] = [s.strip() for s in skills_value.split(',') if s.strip()] # Attempt to parse comma-separated string into list for metadata\n",
        "                 if skills_value.strip(): # Add to content only if not empty string\n",
        "                    profile_text_parts.append(f\"Skills: {skills_value}\")\n",
        "            else:\n",
        "                 metadata['Skills'] = [] # Ensure Skills metadata is always a list\n",
        "\n",
        "            # Add other fields\n",
        "            for field in ['Name', 'Email', 'Location', 'Phone', 'WorkExperience', 'major']: # Exclude 'Skills' and 'Education' as handled above\n",
        "                value = doc_data.get(field)\n",
        "                if value:\n",
        "                    if isinstance(value, (str, int, float)): # Only add if standard type\n",
        "                        if isinstance(value, str) and not value.strip(): continue # Skip empty strings\n",
        "                        profile_text_parts.append(f\"{field}: {value}\")\n",
        "                    else:\n",
        "                         print(f\"[WARN] Skipping field '{field}' for ID {alumni_id} due to unexpected type: {type(value)}\")\n",
        "\n",
        "            profile_text = \"\\n\".join(profile_text_parts)\n",
        "\n",
        "            if profile_text.strip():\n",
        "                documents.append(Document(page_content=profile_text, metadata=metadata))\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                print(f\"[WARN] Skipping document with ID {alumni_id}: No relevant text fields found after processing.\")\n",
        "\n",
        "        print(f\"[INFO] Finished streaming documents. Total documents read from Firestore: {doc_count}\")\n",
        "        print(f\"✅ Loaded {len(documents)} documents for processing (Processed count: {processed_count}).\")\n",
        "        return documents\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading data from Firestore: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# --- Enhanced Vector Store Setup ---\n",
        "def setup_vector_store(force_rebuild: bool = False):\n",
        "    \"\"\"Enhanced vector store setup with better chunk processing.\"\"\"\n",
        "    global vector_store\n",
        "    print(\"[INFO] Setting up vector store...\")\n",
        "    if embeddings is None:\n",
        "        print(\"❌ Cannot setup vector store: Embeddings not initialized.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "    print(f\"[INFO] Ensuring persistence directory exists: {VECTOR_DB_PATH}\")\n",
        "\n",
        "    # Try loading existing store\n",
        "    if not force_rebuild:\n",
        "        if os.path.exists(VECTOR_DB_PATH) and any(os.scandir(VECTOR_DB_PATH)): # Check if directory exists and is not empty\n",
        "            print(f\"[INFO] Attempting to load existing vector store from {VECTOR_DB_PATH}...\")\n",
        "            try:\n",
        "                vector_store = Chroma(\n",
        "                    persist_directory=VECTOR_DB_PATH,\n",
        "                    embedding_function=embeddings\n",
        "                )\n",
        "                # Test loading a document to check if the store is healthy\n",
        "                try:\n",
        "                    # Perform a simple query to check if the collection is accessible and has embeddings\n",
        "                    # This is more robust than just counting files.\n",
        "                    test_docs = vector_store.similarity_search(\"test query\", k=1)\n",
        "                    count = vector_store._collection.count() if hasattr(vector_store, '_collection') else -1 # Fallback count\n",
        "                    if count > 0:\n",
        "                        print(f\"✅ Vector store loaded successfully with {count} documents.\")\n",
        "                        return\n",
        "                    else:\n",
        "                        print(\"⚠️ Existing vector store found, but it appears empty or unhealthy. Rebuilding...\")\n",
        "                        # Fall through to rebuild logic\n",
        "                except Exception as test_e:\n",
        "                    print(f\"❌ Existing vector store failed health check: {test_e}. Rebuilding...\")\n",
        "                    # Fall through to rebuild logic\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading vector store from {VECTOR_DB_PATH}: {e}\")\n",
        "                traceback.print_exc()\n",
        "                print(\"⚠️ Loading failed. Attempting to rebuild vector store.\")\n",
        "                # Fall through to rebuild logic\n",
        "        else:\n",
        "            print(f\"[INFO] No existing vector store found at {VECTOR_DB_PATH} or directory is empty. Building new one.\")\n",
        "\n",
        "    # Build new store\n",
        "    print(\"[INFO] Building new vector store from Firestore data...\")\n",
        "    if firestore_db is None:\n",
        "        print(\"❌ Cannot build new vector store: Firestore connection not initialized.\")\n",
        "        vector_store = None # Ensure vector_store is None if build fails\n",
        "        return\n",
        "\n",
        "    alumni_documents = load_data_from_firestore()\n",
        "    if not alumni_documents:\n",
        "        print(\"⚠️ No documents loaded from Firestore to build vector store.\")\n",
        "        vector_store = None # Ensure vector_store is None if build fails\n",
        "        return\n",
        "\n",
        "    all_chunks = []\n",
        "    print(f\"[INFO] Chunking {len(alumni_documents)} documents...\")\n",
        "    for i, doc in enumerate(alumni_documents):\n",
        "        # print(f\"[INFO] Chunking document {i+1}/{len(alumni_documents)} (ID: {doc.metadata.get('alumni_id')})...\")\n",
        "        chunks = chunk_text(doc.page_content)\n",
        "        if chunks:\n",
        "             # Pass relevant metadata to chunks\n",
        "             for chunk in chunks:\n",
        "                 # Ensure metadata is a copy so modifications per chunk don't affect original doc metadata\n",
        "                 chunk.metadata = {\n",
        "                     **doc.metadata, # Include all original metadata\n",
        "                     'chunk_index': i, # Index of the chunk within the original document's chunks\n",
        "                     'content_length': len(chunk.page_content)\n",
        "                 }\n",
        "             all_chunks.extend(chunks)\n",
        "        else:\n",
        "            print(f\"[WARN] No chunks created for document ID: {doc.metadata.get('alumni_id')}\")\n",
        "\n",
        "    if not all_chunks:\n",
        "        print(\"⚠️ No chunks created from documents. Vector store will be empty.\")\n",
        "        vector_store = None # Ensure vector_store is None if build fails\n",
        "        return\n",
        "\n",
        "    print(f\"[INFO] Adding {len(all_chunks)} chunks to Chroma DB...\")\n",
        "    try:\n",
        "        # If rebuilding, clean up the old directory first\n",
        "        if os.path.exists(VECTOR_DB_PATH):\n",
        "             print(f\"[INFO] Clearing existing directory before rebuild: {VECTOR_DB_PATH}\")\n",
        "             # Use ignore_errors=True in case of permission issues, though ideally fix permissions\n",
        "             shutil.rmtree(VECTOR_DB_PATH, ignore_errors=True)\n",
        "             os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "\n",
        "        vector_store = Chroma.from_documents(\n",
        "            documents=all_chunks,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=VECTOR_DB_PATH\n",
        "        )\n",
        "        vector_store.persist() # Explicitly persist changes\n",
        "        print(f\"✅ Vector store created and persisted with {len(all_chunks)} chunks.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error building vector store: {e}\")\n",
        "        traceback.print_exc()\n",
        "        vector_store = None # Ensure vector_store is None if build fails\n",
        "\n",
        "# --- Query Expansion and Enhancement ---\n",
        "def expand_query(original_query: str) -> List[str]:\n",
        "    \"\"\"Generate multiple query variations to improve retrieval.\"\"\"\n",
        "    # This is used for RAG *query expansion* after filtering is applied.\n",
        "    # The goal is to find documents relevant to the core question, not the filters.\n",
        "    print(f\"[INFO] Expanding query for RAG retrieval: '{original_query}'\")\n",
        "    variations = [original_query]\n",
        "\n",
        "    # Identify potential keywords or entities in the *cleaned* query for expansion\n",
        "    # Basic example: look for common tech/academic terms and names\n",
        "    # Exclude terms that might have been filters if the cleaning wasn't perfect\n",
        "    keywords = re.findall(r'\\b(python|java|c\\+\\+|machine learning|data science|computer science|electrical engineering|business|finance|healthcare|tech|cloud computing|renewable energy|embedded systems|skills|experience|education|work|profile|contact|location|major)\\b', original_query, re.IGNORECASE)\n",
        "    names = re.findall(r'\\b(?:Mr\\.|Ms\\.|Dr\\.)?\\s*([A-Z][a-z]+(?:\\s[A-Z][a-z]+)+)\\b', original_query) # Basic name detection\n",
        "\n",
        "    if keywords:\n",
        "        # print(f\"[INFO] Detected potential keywords for expansion: {list(set(keywords))}\")\n",
        "        for keyword in list(set(keywords)): # Use set to avoid duplicates\n",
        "             variations.extend([\n",
        "                 f\"Alumni with expertise in {keyword}\",\n",
        "                 f\"Profiles mentioning {keyword}\",\n",
        "                 f\"Information about {keyword} skills/experience\" # Added skill/experience angle\n",
        "             ])\n",
        "\n",
        "    if names:\n",
        "        # print(f\"[INFO] Detected potential names for expansion: {list(set(names))}\")\n",
        "        for name in list(set(names)): # Use set to avoid duplicates\n",
        "            variations.extend([\n",
        "                f\"Profile details for {name}\",\n",
        "                f\"Information about {name}'s background\",\n",
        "                f\"{name}'s skills and experience\" # Added skill/experience angle\n",
        "            ])\n",
        "\n",
        "    # Add general variations\n",
        "    simple_variations = [\n",
        "        f\"Search results for {original_query}\", # More keyword-oriented\n",
        "        f\"Relevant information for {original_query}\"\n",
        "    ]\n",
        "    variations.extend([v for v in simple_variations if v not in variations]) # Avoid duplicates\n",
        "\n",
        "    # Deduplicate the final list while preserving order roughly\n",
        "    final_variations = []\n",
        "    seen = set()\n",
        "    for v in variations:\n",
        "        if v not in seen:\n",
        "            final_variations.append(v)\n",
        "            seen.add(v)\n",
        "\n",
        "    # print(f\"[INFO] Generated {len(final_variations)} query variations for RAG.\")\n",
        "    return final_variations\n",
        "\n",
        "# --- Core Retrieval Logic (used by the wrapper) ---\n",
        "def perform_filtered_retrieval(query: str, metadata_filter: Optional[Dict] = None, k: int = 10) -> List[Document]:\n",
        "    \"\"\"Performs similarity search with metadata filtering and logs.\"\"\"\n",
        "    print(f\"[INFO] Searching vector store with query: '{query}' and filter: {metadata_filter}\")\n",
        "    if vector_store is None:\n",
        "         print(\"❌ Search failed: Vector store not initialized.\")\n",
        "         return []\n",
        "    try:\n",
        "        # Retrieve more documents than needed before re-ranking if re-ranking is desired here\n",
        "        # Our wrapper combines results from expanded queries and does a final sort,\n",
        "        # so k is the number of docs per query variation.\n",
        "        docs = vector_store.similarity_search(query, k=k, filter=metadata_filter)\n",
        "        # print(f\"[INFO] Retrieved {len(docs)} docs for query: '{query}'\")\n",
        "        return docs\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during similarity search: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# --- Custom Retriever Class for Langchain ---\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from typing import List\n",
        "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
        "\n",
        "class CustomFilteredRetriever(BaseRetriever):\n",
        "    vectorstore: Chroma\n",
        "    k: int = 10 # Number of documents to return overall\n",
        "    k_per_query: int = 5 # Number of documents to retrieve per expanded query variation\n",
        "\n",
        "    # Ensure _get_relevant_documents accepts the filter kwarg\n",
        "    def _get_relevant_documents(\n",
        "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, filter: Optional[Dict] = None\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Retrieve documents based on the query and metadata filter.\n",
        "        Incorporates query expansion and combines/re-ranks results.\n",
        "        \"\"\"\n",
        "        print(f\"[INFO] CustomFilteredRetriever called with query: '{query}' and filter: {filter}\")\n",
        "\n",
        "        # Perform query expansion on the query received from the RAG chain\n",
        "        expanded_queries = expand_query(query)\n",
        "\n",
        "        all_retrieved_docs = []\n",
        "        seen_doc_chunk_ids = set()\n",
        "\n",
        "        # Perform filtered retrieval for each expanded query variation\n",
        "        for expanded_q in expanded_queries:\n",
        "             # Retrieve k_per_query documents for each variation, applying the filter\n",
        "             docs = perform_filtered_retrieval(expanded_q, filter, k=self.k_per_query)\n",
        "\n",
        "             for doc in docs:\n",
        "                 # Create a unique identifier for each document chunk\n",
        "                 doc_chunk_id = doc.metadata.get('alumni_id', 'unknown') + '_' + str(doc.metadata.get('chunk_index', -1))\n",
        "                 if doc_chunk_id not in seen_doc_chunk_ids:\n",
        "                     all_retrieved_docs.append(doc)\n",
        "                     seen_doc_chunk_ids.add(doc_chunk_id)\n",
        "                     # print(f\"[DEBUG] Added unique doc chunk {doc_chunk_id}\")\n",
        "                 # else:\n",
        "                     # print(f\"[DEBUG] Skipping duplicate doc chunk {doc_chunk_id}\")\n",
        "\n",
        "        print(f\"[INFO] CustomFilteredRetriever collected {len(all_retrieved_docs)} unique documents from {len(expanded_queries)} expanded queries.\")\n",
        "\n",
        "        # Apply final re-ranking if needed across all collected documents\n",
        "        # Re-rank by content length (simple proxy for detail)\n",
        "        if all_retrieved_docs:\n",
        "            final_reranked_docs = sorted(\n",
        "                all_retrieved_docs,\n",
        "                key=lambda x: x.metadata.get('content_length', 0),\n",
        "                reverse=True\n",
        "            )\n",
        "            print(f\"✅ CustomFilteredRetriever returning top {min(self.k, len(final_reranked_docs))} documents after combining and re-ranking.\")\n",
        "            return final_reranked_docs[:self.k] # Return top K overall documents\n",
        "        else:\n",
        "             print(\"⚠️ CustomFilteredRetriever returned no documents.\")\n",
        "             return []\n",
        "\n",
        "# --- Enhanced RAG Chain Setup ---\n",
        "def setup_rag_chain():\n",
        "    \"\"\"Sets up the enhanced RAG chain with better retrieval.\"\"\"\n",
        "    global rag_chain\n",
        "    print(\"[INFO] Setting up RAG chain...\")\n",
        "    if vector_store is None or llm is None:\n",
        "        print(\"❌ Cannot set up RAG chain: Required components (Vector Store or LLM) not initialized.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Instantiate our custom retriever\n",
        "        # Pass the desired total number of documents (k) and docs per query variation (k_per_query)\n",
        "        custom_retriever = CustomFilteredRetriever(\n",
        "             vectorstore=vector_store,\n",
        "             k=10, # Total documents to return to the LLM\n",
        "             k_per_query=5 # Number of documents to get per expanded query variation before combining\n",
        "        )\n",
        "        print(\"✅ Custom filtered retriever created.\")\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\" # Ensure memory key matches output key\n",
        "        )\n",
        "        print(\"✅ Conversation buffer memory initialized.\")\n",
        "\n",
        "        current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "        print(f\"[INFO] Using current date for prompt: {current_date}\")\n",
        "\n",
        "        # Enhanced prompt template\n",
        "        # Make sure the prompt clearly instructs the LLM to use ONLY the provided context.\n",
        "        template = f\"\"\"You are an intelligent Alumni Information Assistant. Current date: {current_date}. Provide answers based ONLY on the provided context documents.\n",
        "\n",
        "When asked about skills:\n",
        "1. ONLY consider explicit mentions in 'Skills' or 'WorkExperience' fields from the context documents.\n",
        "2. For \"skills of [name]\", list ONLY what's explicitly mentioned in the retrieved documents for that person.\n",
        "3. If no relevant documents are retrieved or no skills are explicitly mentioned in the context for the person, state that information is not available or no specific skills were mentioned.\n",
        "\n",
        "For education:\n",
        "1. List ALL degrees and completion dates explicitly stated in the context.\n",
        "2. Do NOT infer degrees or dates not present in the context.\n",
        "\n",
        "For work experience:\n",
        "1. Summarize the work experience as described in the context. Note date ranges if present, relative to {current_date}.\n",
        "2. Do **not** append any “(Information might be outdated)” tags.\n",
        "\n",
        "For contact information (Email, Phone, Location):\n",
        "1. Only provide contact information if it is explicitly present in the context documents.\n",
        "2. If requested information is not in the context, state that it is not available.\n",
        "\n",
        "General Answer Guidelines:\n",
        "- Be precise, factual, and directly answer the user's question using ONLY the provided context.\n",
        "- If the answer is not in the context, politely state that you cannot find the information. Do not make up answers.\n",
        "- Use bullet points or clear formatting for lists (like skills or degrees).\n",
        "- Avoid conversational filler outside of the initial greeting and inability to find information.\n",
        "- Do not mention the source documents or retrieval process in the final answer.\n",
        "- If a query involves criteria (like graduation year or location) that were used to filter the search results, assume the context provided already meets those criteria, and answer based on the *content* within that filtered context. Do not re-verify the filter criteria from the document content itself unless explicitly asked to list all criteria.\n",
        "\n",
        "Chat History:\n",
        "{{chat_history}}\n",
        "\n",
        "Context:\n",
        "{{context}}\n",
        "\n",
        "Question: {{question}}\n",
        "\n",
        "Answer:\"\"\" # Changed 'Answer guidelines:' to 'Answer:' to make the LLM start generating directly\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"chat_history\", \"context\", \"question\"]\n",
        "        )\n",
        "        print(\"✅ Prompt template created.\")\n",
        "\n",
        "        # Create the ConversationalRetrievalChain using our custom retriever\n",
        "        rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=custom_retriever, # Use the custom retriever that handles filters and expansion\n",
        "            memory=memory,\n",
        "            combine_docs_chain_kwargs={\"prompt\": prompt},\n",
        "            return_source_documents=True # Useful for debugging\n",
        "        )\n",
        "        print(\"✅ Enhanced RAG chain setup complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error setting up RAG chain: {e}\")\n",
        "        traceback.print_exc()\n",
        "        rag_chain = None\n",
        "\n",
        "# --- Enhanced Query Processing ---\n",
        "async def query_alumni_bot(user_query: str, history: List[Tuple[str, str]]):\n",
        "    \"\"\"Enhanced query processing with better filtering and error handling.\"\"\"\n",
        "    print(f\"\\n[INFO] Received user query: '{user_query}'\")\n",
        "    if rag_chain is None:\n",
        "        error_msg = \"⚠️ Chatbot system not ready. Please check logs for initialization errors.\"\n",
        "        print(error_msg)\n",
        "        return \"\", history + [(user_query, error_msg)]\n",
        "\n",
        "    if not user_query or not user_query.strip():\n",
        "        print(\"[INFO] Empty or whitespace-only user query received.\")\n",
        "        return \"\", history\n",
        "\n",
        "    original_query = user_query.strip() # Use a cleaned version for processing\n",
        "\n",
        "    # --- Improved Metadata Filtering Logic ---\n",
        "    # Define filter patterns and corresponding metadata fields/operators\n",
        "    # Order matters here - try specific patterns first.\n",
        "    # Regex breakdown:\n",
        "    # - (?:...) non-capturing group\n",
        "    # - (.+?) non-greedy capture of the value\n",
        "    # - (?:\\s*(?:and|or)\\s*|$) non-capturing group matching ' and', ' or' (with optional surrounding space) or end of string\n",
        "    # Added more flexibility to capture values\n",
        "    filters_list = [\n",
        "        (\"major\", \"$eq\", r\"(?:major is|majored in) (.+?)(?:\\s*(?:and|or)\\s*|$)\"),\n",
        "        (\"Location\", \"$eq\", r\"(?:live in|located in|is in|from) (.+?)(?:\\s*(?:and|or)\\s*|$)\"),\n",
        "        (\"graduation_year_int\", \"$lt\", r\"graduated before (\\d{4})(?:\\s*(?:and|or)\\s*|$)\"),\n",
        "        (\"graduation_year_int\", \"$gt\", r\"graduated after (\\d{4})(?:\\s*(?:and|or)\\s*|$)\"),\n",
        "        # Regex for skills - looks for phrases like \"with skill/skills\", \"experience in\", followed by content\n",
        "        (\"Skills\", \"$in\", r\"(?:with|has) (?:skill(?:s)?|experience in) (.+?)(?:\\s*(?:and|or)\\s*|$)\"),\n",
        "        # Add patterns for specific skills/domains if you want to map them to the Skills metadata field\n",
        "        # e.g., (\"Skills\", \"$in\", r\"(?:\\bpython\\b)(?:\\s*(?:and|or)\\s*|$)\") # Very specific\n",
        "        # A more general approach is to let RAG handle skill details after filtering by other criteria.\n",
        "        # Let's keep the Skills pattern general to capture phrases like \"python, java, cloud\"\n",
        "    ]\n",
        "\n",
        "    metadata_filter = {}\n",
        "    spans_to_remove = []\n",
        "    query_processing_copy = original_query # Work on a copy for matching\n",
        "\n",
        "    print(f\"[INFO] Attempting to extract filters from query: '{original_query}'\")\n",
        "\n",
        "    for field, op, pattern in filters_list:\n",
        "        # Use finditer to get spans of all matches for the current pattern\n",
        "        matches = list(re.finditer(pattern, query_processing_copy, re.IGNORECASE))\n",
        "\n",
        "        for match in matches:\n",
        "            value_str = match.group(1).strip()\n",
        "            start, end = match.span() # Get the span of the full match\n",
        "\n",
        "            try:\n",
        "                value = value_str\n",
        "                # Type conversion based on field\n",
        "                if field == 'graduation_year_int':\n",
        "                     try:\n",
        "                         value = int(value_str)\n",
        "                     except ValueError:\n",
        "                         print(f\"[WARN] Could not convert '{value_str}' to integer for graduation year. Skipping filter.\")\n",
        "                         continue # Skip this filter if value is not an integer\n",
        "                elif field == 'Skills' and op == \"$in\":\n",
        "                     # Assuming comma-separated list for $in operator value\n",
        "                     value = [s.strip() for s in value_str.split(',') if s.strip()]\n",
        "                     if not value: # Skip empty skill lists\n",
        "                         print(f\"[WARN] Extracted empty skill list from '{value_str}'. Skipping filter.\")\n",
        "                         continue # Skip adding this filter\n",
        "\n",
        "                # Add to metadata_filter dictionary\n",
        "                # This logic assumes simple ANDing of different filter fields.\n",
        "                # For the same field, it handles $lt/$gt ranges or accumulates $in values.\n",
        "                # $eq for the same field would overwrite previous $eq for simplicity.\n",
        "                if field not in metadata_filter:\n",
        "                    if op in [\"$lt\", \"$gt\"]:\n",
        "                         metadata_filter[field] = {} # Initialize as dict for range filters\n",
        "                    elif op == \"$in\":\n",
        "                         metadata_filter[field] = {op: []} # Initialize as list for $in\n",
        "                    # else: # $eq will just set the value directly below\n",
        "                        # pass\n",
        "\n",
        "                if op in [\"$lt\", \"$gt\"]:\n",
        "                    # Combine $lt and $gt for the same field\n",
        "                    if field in metadata_filter and isinstance(metadata_filter[field], dict):\n",
        "                        metadata_filter[field][op] = value\n",
        "                    else:\n",
        "                        # Should not happen with initialization above, but handle defensively\n",
        "                        print(f\"[WARN] Filter logic issue: field '{field}' not dict for range filter. Setting directly.\")\n",
        "                        metadata_filter[field] = {op: value}\n",
        "                elif op == \"$in\":\n",
        "                     # Extend the list for $in operator\n",
        "                     if field in metadata_filter and op in metadata_filter[field] and isinstance(metadata_filter[field][op], list):\n",
        "                         metadata_filter[field][op].extend(value) # Extend the list with new values\n",
        "                     else:\n",
        "                          # Should not happen, but handle defensively\n",
        "                          print(f\"[WARN] Filter logic issue: field '{field}' not list for $in filter. Setting directly.\")\n",
        "                          metadata_filter[field] = {op: value}\n",
        "                else: # $eq\n",
        "                    # Set or overwrite $eq filter\n",
        "                    metadata_filter[field] = value\n",
        "\n",
        "                # Record the span of the *full match* including the connector if present\n",
        "                spans_to_remove.append((start, end))\n",
        "                print(f\"[FILTER] Identified filter: Field='{field}', Operator='{op}', Value='{value}' (Span: {start}-{end}, Matched Text: '{match.group(0)}')\") # Log matched text too\n",
        "\n",
        "            except Exception as filter_e:\n",
        "                print(f\"❌ Error processing filter match for pattern '{pattern}': {filter_e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    # Sort spans by start index in reverse order to remove without affecting subsequent indices\n",
        "    spans_to_remove.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Build the cleaned question string by removing identified filter spans\n",
        "    cleaned_query_parts = []\n",
        "    last_end = len(original_query)\n",
        "    for start, end in spans_to_remove:\n",
        "        # Add the segment *before* the current filter span\n",
        "        segment = original_query[end:last_end].strip()\n",
        "        if segment:\n",
        "            cleaned_query_parts.append(segment)\n",
        "        last_end = start # Update last_end to the start of the current filter span\n",
        "\n",
        "    # Add the segment before the first filter (or the whole string if no filters)\n",
        "    segment = original_query[0:last_end].strip()\n",
        "    if segment:\n",
        "         cleaned_query_parts.append(segment)\n",
        "\n",
        "    # Join parts and clean up extra whitespace\n",
        "    # Join in reverse order because spans_to_remove was processed reverse\n",
        "    cleaned_query_parts.reverse()\n",
        "    # Use regex to replace multiple spaces with a single space and strip leading/trailing space\n",
        "    question = re.sub(r'\\s+', ' ', \" \".join(cleaned_query_parts)).strip()\n",
        "\n",
        "    if not question:\n",
        "        print(\"[WARN] Question became empty after removing filters. Using original query for RAG.\")\n",
        "        question = original_query\n",
        "    elif question.lower().strip() == original_query.lower().strip() and not metadata_filter:\n",
        "         print(\"[INFO] No filters were successfully extracted or applied.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Cleaned question for RAG: '{question}'\")\n",
        "\n",
        "    if metadata_filter:\n",
        "        print(f\"[INFO] Active metadata filters passed to retriever: {metadata_filter}\")\n",
        "    else:\n",
        "         print(\"[INFO] No metadata filters were applied.\")\n",
        "\n",
        "    # --- End Improved Metadata Filtering Logic ---\n",
        "\n",
        "    try:\n",
        "        # Invoke the RAG chain.\n",
        "        # Pass the filter via the config dictionary. Langchain's ConversationalRetrievalChain\n",
        "        # should pass this config down to the retriever's `get_relevant_documents` method.\n",
        "        # Our `CustomFilteredRetriever` is designed to accept this 'filter' kwarg.\n",
        "        # We use retriever.with_config() to dynamically add the filter for this specific invoke call.\n",
        "        result = rag_chain.invoke(\n",
        "            {\"question\": question, \"chat_history\": history}, # Pass the cleaned question and history\n",
        "            config={\n",
        "                 \"configurable\": {\n",
        "                       \"retriever\": rag_chain.retriever.with_config( # Use .with_config to add filter to retriever for this call\n",
        "                           search_kwargs={\"filter\": metadata_filter}\n",
        "                       )\n",
        "                 }\n",
        "            }\n",
        "        )\n",
        "        print(\"✅ RAG chain invoke successful.\")\n",
        "        answer = result.get('answer', \"Sorry, I couldn't generate a response based on the available information.\")\n",
        "        source_documents = result.get('source_documents', [])\n",
        "        print(f\"[INFO] Received answer (first 100 chars): '{answer[:100]}'\")\n",
        "        print(f\"[INFO] Retrieved {len(source_documents)} source documents.\")\n",
        "        # for i, doc in enumerate(source_documents):\n",
        "        #     print(f\"[DEBUG] Source {i}: Metadata={doc.metadata}, Content={doc.page_content[:200]}...\")\n",
        "\n",
        "        # Post-process answer for better clarity - remove canned phrases from prompt\n",
        "        answer = re.sub(r\"(?i)based on the provided context,?\", \"\", answer).strip()\n",
        "        answer = re.sub(r\"(?i)according to the context,?\", \"\", answer).strip()\n",
        "        answer = re.sub(r\"(?i)based on the information provided,?\", \"\", answer).strip()\n",
        "        answer = re.sub(r\"(?i)according to the profile,?\", \"\", answer).strip() # From the prompt\n",
        "        answer = re.sub(r\"(?i)information not available\", \"Information not available in the profiles.\", answer).strip() # From the prompt\n",
        "        answer = re.sub(r\"(?i)no specific skills mentioned\", \"No specific skills mentioned in the profiles.\", answer).strip() # From the prompt\n",
        "\n",
        "        # If the answer seems empty or uninformative after cleaning and no docs were retrieved\n",
        "        # Add a check to ensure the answer isn't just whitespace or punctuation\n",
        "        if (not answer or answer.lower() in [\"\", \"sorry, i couldn't generate a response based on the available information.\", \"information not available in the profiles.\", \"no specific skills mentioned in the profiles.\"] or re.fullmatch(r'[^\\w\\s]+', answer)) and not source_documents:\n",
        "             answer = \"Sorry, I could not find relevant information in the alumni profiles for your query.\"\n",
        "             print(\"[INFO] Answer is uninformative and no documents were retrieved. Returning default message.\")\n",
        "\n",
        "        # Append source document metadata (optional, for debugging/transparency)\n",
        "        # source_info = \"\\n\\n---\\nSource Documents:\\n\"\n",
        "        # if source_documents:\n",
        "        #     for doc in source_documents:\n",
        "        #         source_info += f\"- ID: {doc.metadata.get('alumni_id', 'N/A')}, Chunk: {doc.metadata.get('chunk_index', 'N/A')}, Name: {doc.metadata.get('name', 'N/A')}\\n\"\n",
        "        #     answer += source_info\n",
        "\n",
        "        return \"\", history + [(user_query, answer)]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Query processing error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        error_msg = \"⚠️ An internal error occurred while processing your query. Please try again or rephrase.\"\n",
        "        return \"\", history + [(user_query, error_msg)]\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Starting Enhanced Alumni Chatbot ---\")\n",
        "\n",
        "    # Initialize environment (Colab Drive mount or local path)\n",
        "    initial_vector_db_path = VECTOR_DB_PATH # Store initial value\n",
        "    try:\n",
        "        print(\"[INFO] Checking environment...\")\n",
        "        # Check if running in Colab by looking for specific environment variables\n",
        "        if 'COLAB_GPU' in os.environ or 'GOOGLE_COLAB' in os.environ:\n",
        "            print(\"[INFO] Running in Google Colab environment.\")\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                if not os.path.exists('/content/drive'):\n",
        "                    print(\"[INFO] Attempting to mount Google Drive...\")\n",
        "                    drive.mount('/content/drive')\n",
        "                    print(\"✅ Google Drive mounted.\")\n",
        "                else:\n",
        "                    print(\"✅ Google Drive already mounted.\")\n",
        "                os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "                print(f\"✅ Ensured Chroma DB path exists: {VECTOR_DB_PATH}\")\n",
        "            except ImportError:\n",
        "                 print(\"⚠️ `google.colab` not available despite environment indicators. Using local storage.\")\n",
        "                 VECTOR_DB_PATH = \"./chroma_db_local_firestore\"\n",
        "                 os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "                 print(f\"✅ Switched to local Chroma DB path: {VECTOR_DB_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during Google Drive mount: {e}\")\n",
        "                traceback.print_exc()\n",
        "                print(\"⚠️ Drive mount failed. Falling back to local storage.\")\n",
        "                VECTOR_DB_PATH = \"./chroma_db_local_firestore\"\n",
        "                os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "                print(f\"✅ Switched to local Chroma DB path: {VECTOR_DB_PATH}\")\n",
        "        else:\n",
        "            print(\"[INFO] Not running in Google Colab. Using local storage for Chroma DB.\")\n",
        "            if initial_vector_db_path.startswith(\"/content/drive/\"):\n",
        "                 # If the default path was a Drive path but we're not in Colab\n",
        "                 VECTOR_DB_PATH = \"./chroma_db_local_firestore\"\n",
        "                 print(f\"⚠️ Adjusted Chroma DB path from Drive path to local: {VECTOR_DB_PATH}\")\n",
        "            os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "            print(f\"✅ Ensured local Chroma DB path exists: {VECTOR_DB_PATH}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error during environment check: {e}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"⚠️ Proceeding with potential default local path.\")\n",
        "        if VECTOR_DB_PATH.startswith(\"/content/drive/\"):\n",
        "             VECTOR_DB_PATH = \"./chroma_db_local_firestore\"\n",
        "             print(f\"⚠️ Adjusted Chroma DB path due to error: {VECTOR_DB_PATH}\")\n",
        "        os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "        print(f\"✅ Ensured Chroma DB path exists: {VECTOR_DB_PATH}\")\n",
        "\n",
        "    # Initialize components\n",
        "    print(\"\\n[INFO] --- Initializing Components ---\")\n",
        "    load_google_api_key()\n",
        "    initialize_database()\n",
        "    initialize_embeddings()\n",
        "\n",
        "    # Check if embeddings initialized before setting up vector store\n",
        "    if embeddings is None:\n",
        "        print(\"[ERROR] Embeddings failed to initialize. Cannot proceed with vector store setup or RAG chain.\")\n",
        "    else:\n",
        "        # Set force_rebuild=True ONLY if you want to rebuild the vector store from Firestore every time\n",
        "        # Set to False to load from the persistence directory if available.\n",
        "        setup_vector_store(force_rebuild=False)\n",
        "\n",
        "        # Check if LLM initialized before setting up RAG chain\n",
        "        initialize_llm()\n",
        "        if llm is None:\n",
        "             print(\"[ERROR] LLM failed to initialize. Cannot proceed with RAG chain setup.\")\n",
        "        elif vector_store is None:\n",
        "             print(\"[ERROR] Vector store failed to initialize. Cannot proceed with RAG chain setup.\")\n",
        "        else:\n",
        "            setup_rag_chain()\n",
        "\n",
        "    print(\"[INFO] --- Initialization Complete ---\")\n",
        "\n",
        "    # Launch Gradio interface\n",
        "    if rag_chain:\n",
        "        print(\"\\n[INFO] Launching Gradio interface...\")\n",
        "        with gr.Blocks(theme=gr.themes.Soft()) as app: # Added a simple theme\n",
        "            gr.Markdown(\"# Alumni Information Chatbot\")\n",
        "            gr.Markdown(\"Ask questions about alumni profiles. Examples: 'What are Virginia Hammond's skills?', 'Find alumni who graduated after 2020 and live in California.'\")\n",
        "\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"Chat History\",\n",
        "                height=500, # Increased height for better visibility\n",
        "                layout=\"bubble\", # Use bubble layout\n",
        "                # avatar_images=((os.path.join(os.path.dirname(__file__), \"user.png\") if '__file__' in locals() else None),\n",
        "                #                (os.path.join(os.path.dirname(__file__), \"bot.png\") if '__file__' in locals() else None)), # Example avatar paths - requires files\n",
        "                value=[(None, \"Hello! I can help you find information about alumni. Ask me anything about their skills, education, or work experience.\")]\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(label=\"Your Question\", placeholder=\"Enter your question here...\", scale=4, autofocus=True)\n",
        "                send_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear = gr.Button(\"Clear Chat\")\n",
        "                # Add a button to force rebuild the vector store (optional, for testing)\n",
        "                rebuild_btn = gr.Button(\"Force Rebuild DB\")\n",
        "\n",
        "            # Comprehensive example questions\n",
        "            example_queries = [\n",
        "                 \"Tell me everything you know about Virginia Hammond.\",\n",
        "                 \"What are Virginia Hammond's key skills?\",\n",
        "                 \"What is Virginia Hammond's contact information?\",\n",
        "                 \"Tell me about Paul Walker's work experience.\",\n",
        "                 \"Did Paul Walker study Computer Science?\", # Will likely be RAG only unless 'major is computer science' filter is added\n",
        "                 \"Find alumni with Python programming skills.\", # This will now attempt filter ($in Skills) + RAG\n",
        "                 \"List alumni with experience in machine learning.\", # This will now attempt filter ($in Skills) + RAG\n",
        "                 \"Who has experience with cloud computing?\", # This will now attempt filter ($in Skills) + RAG\n",
        "                 \"Find alumni skilled in data analysis.\", # This will now attempt filter ($in Skills) + RAG\n",
        "                 \"Which alumni know JavaScript?\", # This will now attempt filter ($in Skills) + RAG\n",
        "                 \"What are the key skills or areas of expertise mentioned for alumni who studied Computer Science?\", # This will use filter (major if pattern matches) + RAG for skills\n",
        "                 \"List alumni who majored in Electrical Engineering.\", # This will use filter (major)\n",
        "                 \"Find alumni who graduated before 2015.\", # This will use filter ($lt graduation_year_int)\n",
        "                 \"Who completed their degree after 2018?\", # This will use filter ($gt graduation_year_int)\n",
        "                 \"Find alumni who live in California.\", # This will use filter ($eq Location)\n",
        "                 \"Who is located in New York?\", # This will use filter ($eq Location)\n",
        "                 \"List alumni in Texas.\", # This will use filter ($eq Location)\n",
        "                 \"Find alumni who graduated before 2020 and live in California.\", # This will use filters ($lt graduation_year_int, $eq Location)\n",
        "                 \"List Computer Science majors with Python skills.\", # This will use filter (major) + filter ($in Skills)\n",
        "                 \"Find alumni in tech with more than 5 years of experience.\", # RAG only (experience years not a filter)\n",
        "                 \"Find alumni with AI skills who graduated after 2015.\", # This will use filter ($in Skills) + filter ($gt graduation_year_int)\n",
        "                 \"Can you find someone with experience in renewable energy or embedded systems?\", # This will use filter ($in Skills) + RAG\n",
        "            ]\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=example_queries,\n",
        "                inputs=msg,\n",
        "                outputs=[msg, chatbot],\n",
        "                fn=query_alumni_bot,\n",
        "                cache_examples=False, # Set to True if your function is fast\n",
        "                label=\"Suggested Questions (Click any example to try):\"\n",
        "            )\n",
        "\n",
        "            # Connect actions\n",
        "            msg.submit(query_alumni_bot, [msg, chatbot], [msg, chatbot], queue=False) # Use queue=False for quicker response feel\n",
        "            send_btn.click(query_alumni_bot, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "            clear.click(lambda: gr.Chatbot.update(value=[(None, \"Hello! I can help you find information about alumni. Ask me anything about their skills, education, or work experience.\")]), None, chatbot, queue=False)\n",
        "            rebuild_btn.click(lambda: setup_vector_store(force_rebuild=True), None, None, queue=False) # Add action for rebuild button\n",
        "\n",
        "        # Launch the app\n",
        "        # debug=True can help see more detailed logs from Gradio/backend\n",
        "        # share=True generates a public URL (useful for Colab)\n",
        "        app.launch(share=True, debug=True)\n",
        "    else:\n",
        "        print(\"\\n[ERROR] Chatbot components failed to initialize. Gradio interface will not launch. Please check logs for specific errors during initialization.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}