{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FCwVLfGlJSI9"
      },
      "outputs": [],
      "source": [
        "# ðŸ”§ STEP 1: Install required libraries\n",
        "!pip install qdrant-client sentence-transformers google-generativeai --quiet\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "QDRANT_API_KEY = \"QDRANT_API_KEY\"  # ðŸ”’ Paste your API key from Qdrant dashboard\n",
        "QDRANT_URL = \"QDRANT_URL\"\n",
        "\n",
        "client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdK_cclkLQg2",
        "outputId": "3a5eba51-85c6-403e-9221-7ce918484ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model (required for chat)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model loaded.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"Loading embedding model (required for chat)...\")\n",
        "# This line loads the model and assigns it to the 'embedder' variable\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedding model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61VTeJHWNMpu",
        "outputId": "4907d0fd-8df6-4a29-e3ac-9bdaf8a27013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL_ENDPOINT_URL:  https://11fc-34-126-176-47.ngrok-free.app/predict\n",
            "--- Alumni Profile Chat Bot ---\n",
            "Ask questions about the alumni profiles. Type 'quit' or 'exit' to end.\n",
            "\n",
            "You: What is the phone number of NAVARRO?\n",
            "Searching knowledge base...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-08eef9d5e551>:40: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = client.search( # Replace with client.query_points if preferred\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 5 potential profiles.\n",
            "Building context...\n",
            "Context built (Length: 3310 chars).\n",
            "Generating response...\n",
            "\n",
            "Assistant: +18034211100 \n"
          ]
        }
      ],
      "source": [
        "# ðŸ’¬ STEP 9: Interactive Chat Loop\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from tqdm import tqdm # Can remove if you don't want per-hit progress during context building\n",
        "\n",
        "# --- Ensure previous initializations are done ---\n",
        "# Make sure 'client', 'embedder', 'LLM_URL', 'API_ENDPOINT', 'headers' are defined.\n",
        "# Define MODEL_ENDPOINT_URL if not already done\n",
        "if 'API_ENDPOINT' not in locals(): API_ENDPOINT = \"/predict\" # Set default if needed\n",
        "if 'LLM_URL' not in locals(): LLM_URL = \"https://11fc-34-126-176-47.ngrok-free.app\" # Replace if needed\n",
        "MODEL_ENDPOINT_URL = LLM_URL.strip('/') + \"/\" + API_ENDPOINT.strip('/')\n",
        "print('MODEL_ENDPOINT_URL: ',MODEL_ENDPOINT_URL)\n",
        "# --- Configuration for the Chat ---\n",
        "SEARCH_LIMIT = 5 # How many results to fetch from Qdrant (adjust as needed)\n",
        "REQUEST_TIMEOUT = 90 # Seconds to wait for LLM response\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "    # Add \"Authorization\": \"Bearer YOUR_KEY\" if your endpoint needs it\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"\\nYou: \")\n",
        "    if not user_query or user_query.lower() in ['quit', 'exit']:\n",
        "        print(\"Exiting chat. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # --- Stage 1: Retrieval (Adapted from Cell 7) ---\n",
        "    print(\"Searching knowledge base...\")\n",
        "    results = []\n",
        "    top_k_context = \"No context retrieved.\" # Default\n",
        "    try:\n",
        "        query_vec = embedder.encode(user_query).tolist()\n",
        "        # Use query_points (recommended) or search\n",
        "        results = client.search( # Replace with client.query_points if preferred\n",
        "            collection_name=\"alumni_profiles\",\n",
        "            query_vector=query_vec,\n",
        "            limit=SEARCH_LIMIT\n",
        "        )\n",
        "        print(f\"Retrieved {len(results)} potential profiles.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Qdrant search: {e}\")\n",
        "        continue # Skip to next loop iteration\n",
        "\n",
        "    # --- Stage 2: Context Building (Adapted from Cell 7 - Comprehensive) ---\n",
        "    print(\"Building context...\")\n",
        "    context_parts = []\n",
        "    if results:\n",
        "        # Using tqdm here might be noisy in chat, consider removing loop prints/tqdm\n",
        "        for i, hit in enumerate(results): # tqdm(results, desc=\"Processing hits\", leave=False):\n",
        "            if not hit.payload: continue\n",
        "\n",
        "            data = hit.payload\n",
        "            name = data.get('Name', 'N/A')\n",
        "            location = data.get('Location', '')\n",
        "            work_exp_list = data.get('WorkExperience', [])\n",
        "            education_list = data.get('Education', [])\n",
        "            skills_list = data.get('Skills', [])\n",
        "            email = data.get('Email', '')\n",
        "            phone = data.get('Phone', '')\n",
        "\n",
        "            if not isinstance(work_exp_list, list): work_exp_list = []\n",
        "            if not isinstance(education_list, list): education_list = []\n",
        "            if not isinstance(skills_list, list): skills_list = []\n",
        "            safe_work_exp = [str(item) for item in work_exp_list]\n",
        "            safe_education = [str(item) for item in education_list]\n",
        "            safe_skills = [str(item) for item in skills_list]\n",
        "\n",
        "            context_part = f\"\"\"Profile Entry {i+1}:\n",
        "            Name: {str(name)}\n",
        "            Location: {str(location)}\n",
        "            Work Experience: {'; '.join(safe_work_exp)}\n",
        "            Education: {'; '.join(safe_education)}\n",
        "            Skills: {', '.join(safe_skills)}\n",
        "            Email: {str(email)}\n",
        "            Phone: {str(phone)}\n",
        "            Search Score: {hit.score:.4f}\n",
        "            \"\"\"\n",
        "            context_parts.append(context_part.strip())\n",
        "\n",
        "        top_k_context = \"\\n\\n=====\\n\\n\".join(context_parts)\n",
        "        print(f\"Context built (Length: {len(top_k_context)} chars).\")\n",
        "    else:\n",
        "        print(\"No relevant profiles found in knowledge base.\")\n",
        "        # Optional: Still send to LLM without context, or just reply directly\n",
        "        # print(\"Assistant: I couldn't find any relevant information in the profiles.\")\n",
        "        # continue\n",
        "\n",
        "    # --- Stage 3: Generation (Adapted from Cell 8 - Custom LLM) ---\n",
        "    print(\"Generating response...\")\n",
        "    final_prompt = f\"\"\"\n",
        "Use the following context to answer the user's question accurately based only on the provided text. If the answer is not found in the context, say so clearly.\n",
        "\n",
        "Context:\n",
        "{top_k_context}\n",
        "\n",
        "Question: {user_query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    payload = {\"text\": final_prompt}\n",
        "    generated_text = \"Sorry, I encountered an error.\" # Default error\n",
        "\n",
        "    try:\n",
        "        response = requests.post(MODEL_ENDPOINT_URL, headers=headers, json=payload, timeout=REQUEST_TIMEOUT)\n",
        "        response.raise_for_status()\n",
        "        response_data = response.json()\n",
        "        try:\n",
        "            generated_text = response_data[\"response\"]\n",
        "        except KeyError:\n",
        "            generated_text = f\"Error: 'response' key missing in LLM output. Raw: {str(response_data)[:200]}\"\n",
        "        except Exception as e:\n",
        "             generated_text = f\"Error parsing LLM response content: {e}\"\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        generated_text = \"Sorry, the request to the language model timed out.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        generated_text = f\"Sorry, error communicating with the language model: {e}\"\n",
        "    except Exception as e:\n",
        "        generated_text = f\"Sorry, an unexpected error occurred: {e}\"\n",
        "\n",
        "    print(f\"\\nAssistant: {generated_text}\")\n",
        "\n",
        "# Suggested example questions\n",
        "suggestions = [\n",
        "    \"What is Virginia Hammond's contact information?\",\n",
        "    \"Find alumni with Python programming skills.\",\n",
        "    \"Find alumni who live in California.\",\n",
        "    \"Find alumni who live in California and have MSc in Computer Science education\",\n",
        "    \"Who completed their degree after 2018 and have MSc in Computer Science ?\",\n",
        "    \"List alumni who majored in Electrical Engineering.\",\n",
        "    \"who graduated with computer science degree around 2020 and works in data science domain?\",\n",
        "    \"Who has experience with cloud computing?\",\n",
        "    \"Who graduated between 2018 and 2021 ? share their degree major ?\",\n",
        "    \"can you give information about John carter ?\"\n",
        "]\n",
        "# Launch Gradio App\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## ðŸŽ“ Alumni Information Chatbot\")\n",
        "    gr.Markdown(\"Ask questions about alumni profiles. Examples: `What are Virginia Hammondâ€™s skills?`, `Find alumni who graduated after 2020 and live in California.`\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat History\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(placeholder=\"Enter your question here...\", label=\"Your Question\", scale=9)\n",
        "        send_btn = gr.Button(\"Send\", scale=1)\n",
        "        clear_btn = gr.Button(\"ðŸ§¹ Clear Chat\")\n",
        "\n",
        "    gr.Markdown(\"**âš¡ Suggested Questions (Click any example to try):**\")\n",
        "\n",
        "    with gr.Row():\n",
        "        for q in suggestions[:5]:\n",
        "            gr.Button(q).click(fn=lambda x=q: x, outputs=txt)\n",
        "    with gr.Row():\n",
        "        for q in suggestions[5:]:\n",
        "            gr.Button(q).click(fn=lambda x=q: x, outputs=txt)\n",
        "\n",
        "    def clear():\n",
        "        return [], \"\"\n",
        "\n",
        "    send_btn.click(chat_bot_fn, inputs=[txt, chatbot], outputs=[chatbot, txt])\n",
        "    txt.submit(chat_bot_fn, inputs=[txt, chatbot], outputs=[chatbot, txt])\n",
        "    clear_btn.click(clear, outputs=[chatbot, txt])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
